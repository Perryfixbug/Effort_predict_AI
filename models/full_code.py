import numpy as np
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import KFold

# X√¢y d·ª±ng model cho Linear Regession
class LinearRegression:
    def __init__(self, alpha):
        self.alpha = alpha  # H·ªá s·ªë ƒëi·ªÅu chu·∫©n Œª
        self.beta = None
        self.poly_transformer = None # L∆∞u c√°c ƒë·∫∑c tr∆∞ng phi tuy·∫øn t·ª´ c√°c ƒë·∫∑c tr∆∞ng g·ªëc.
        self.selector = None  # L∆∞u b·ªô ƒë·∫∑c tr∆∞ng t·ªëi ∆∞u ƒë∆∞·ª£c ch·ªçn
        self.scaler = StandardScaler()

    def _add_bias(self, X):
        return np.c_[np.ones((X.shape[0], 1)), X]
    def print_beta(self):
        return self.beta

    def fit(self, X, y):
        # Chu·∫©n h√≥a X
        X_scaled = self.scaler.fit_transform(X)
        X_b = self._add_bias(X_scaled)

        y = y.reshape(-1, 1) if y.ndim == 1 else y
        m, n = X_b.shape

        # T√≠nh œâ* = (X^T X + Œ±I)^(-1) X^T y
        I = np.eye(n)
        I[0, 0] = 0  # Kh√¥ng regularize bias
        self.beta = np.linalg.inv(X_b.T @ X_b + self.alpha * I) @ X_b.T @ y

    def predict(self, X):
        X_scaled = self.scaler.transform(X)
        X_b = self._add_bias(X_scaled)
        return X_b @ self.beta

    def score(self, X, y_true):
        y_pred = self.predict(X)
        y_true = y_true.reshape(-1, 1) if y_true.ndim == 1 else y_true
        return r2_score(y_true, y_pred)

# X√¢y d·ª±ng thu·∫≠t to√°n ridge regression ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh
def ridge_regression(X_train, y_train, X_test, y_test, use_feature_selection=False):
    # B∆∞·ªõc 1: Bi·∫øn ƒë·ªïi ƒëa th·ª©c
    poly = PolynomialFeatures(degree=2, include_bias=False)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    selector = None
    if use_feature_selection:
        # Ch·ªçn ra 9 ƒë·∫∑c tr∆∞ng c√≥ m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng nh·∫•t ƒë·∫øn nh√£n th√¥ng qua h√†m ƒë√°nh gi√° ƒëi·ªÉm f_regression
        selector = SelectKBest(score_func=f_regression, k=9)
        X_train_poly = selector.fit_transform(X_train_poly, y_train.ravel())
        X_test_poly = selector.transform(X_test_poly)

    # B∆∞·ªõc 2: Cross-validation v·ªõi m√¥ h√¨nh LinearRegression t·ª± x√¢y
    # T√¨m h·ªá s·ªë alpha t·ªëi ∆∞u nh·∫•t
    alphas = np.logspace(-3, 3, 20)
    best_alpha = None
    best_r2 = -np.inf

    kf = KFold(n_splits=5, shuffle=True, random_state=42)

    for alpha in alphas:
        r2_scores = []
        for train_idx, val_idx in kf.split(X_train_poly):
            X_tr, X_val = X_train_poly[train_idx], X_train_poly[val_idx]
            y_tr, y_val = y_train[train_idx], y_train[val_idx]

            model = LinearRegression(alpha=alpha)
            model.fit(X_tr, y_tr)
            r2 = model.score(X_val, y_val)
            r2_scores.append(r2)

        avg_r2 = np.mean(r2_scores)
        if avg_r2 > best_r2:
            best_r2 = avg_r2
            best_alpha = alpha

    print(f"\nüîç Best alpha: {best_alpha:.4f}")

    # B∆∞·ªõc 3: Train l·∫°i m√¥ h√¨nh t·ªët nh·∫•t & L∆∞u l·∫°i c√°c ƒë·∫∑c t√≠nh ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ ti·ªán cho t√≠nh to√°n
    final_model = LinearRegression(alpha=best_alpha)
    final_model.poly_transformer = poly
    final_model.selector = selector
    final_model.fit(X_train_poly, y_train)
    print("Gi√° tr·ªã h·ªá s·ªë beta:")
    print(final_model.print_beta())

    return final_model, selector, poly
